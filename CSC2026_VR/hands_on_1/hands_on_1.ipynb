{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e76ea9fb-7249-4638-9db8-d4f4817a3548",
   "metadata": {},
   "source": [
    "# Hands-On 1 ‚Äî Deep Shielding and Variance Reduction with Geant4\n",
    "\n",
    "**47th School of Computing ‚Äì Latin America 2026**  \n",
    "Hands-On Session 1\n",
    "\n",
    "**Lecturer:** Jaime Romero-Barrientos   \n",
    "**Contact:** jaime.romero@cern.ch   \n",
    "Researcher ‚Äî Chilean Nuclear Energy Commission (CCHEN)  \n",
    "Researcher ‚Äî Millennium Institute for Subatomic Physics at the High-Energy Frontier (SAPHIR)\n",
    "\n",
    "<div style=\"display:flex; align-items:center; gap:32px;\">\n",
    "  <img src=\"https://cernbox.cern.ch/remote.php/dav/public-files/3XD5887poIjRXe6/logo_CCHEN.jpg\" width=\"180\"/>\n",
    "  <img src=\"https://cernbox.cern.ch/remote.php/dav/public-files/foYapCY948XIY91/Logo-Saphir.png\" width=\"200\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30111f46-907f-4f97-910b-72aad7f47402",
   "metadata": {},
   "source": [
    "---\n",
    "## Overview <a name=\"overview\"></a>\n",
    "\n",
    "This hands-on introduces Monte Carlo simulation techniques for **deep shielding problems** using **Geant4**, with emphasis on:\n",
    "\n",
    "- Deep-penetration, rare-event observables\n",
    "- Statistical uncertainty and convergence\n",
    "- Variance reduction techniques\n",
    "- Performance assessment using the Figure of Merit (FOM)\n",
    "\n",
    "You will run, analyze, and compare **analog** and **variance-reduced** simulations for a fixed physical problem, learning how statistical efficiency can be  improved **without modifying the underlying physics**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d39545f-c0ac-433b-af18-5322642f8ca0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "---\n",
    "## Computing environment <a name=\"computing-environment\"></a>\n",
    "\n",
    "This hands-on is executed using:\n",
    "\n",
    "- **Geant4 version:** 11.3.2  \n",
    "- **Physics list:** FTFP_BERT_HP  \n",
    "- **Computing platform:** SWAN (CERN Jupyter service)  \n",
    "- **Storage:** CERNBox  \n",
    "- **Software stack:** LCG 108 (LHC Computing Grid)\n",
    "\n",
    "All simulations are run inside the **SWAN environment**, using CPU resources and a controlled software stack, to ensure a **common and reproducible setup** for all participants."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0967dd-003a-4960-9a9d-696f10d9e75b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "---\n",
    "## Physical problem: deep neutron shielding <a name=\"physical-problem-deep-neutron-shielding\"></a>\n",
    "\n",
    "In this hands-on we study **neutron transport through a thick concrete shield**, a prototypical **deep shielding problem** in radiation transport.\n",
    "\n",
    "In such problems, the quantities of interest are dominated by **rare particle histories** that survive many interactions and reach deep regions of the shield.  \n",
    "This makes the estimation of physical observables **statistically challenging** and an ideal testbed for Monte Carlo methods.\n",
    "\n",
    "---\n",
    "\n",
    "## Geometry and particle transport <a name=\"geometry-and-particle-transport\"></a>\n",
    "\n",
    "The simulated geometry consists of:\n",
    "\n",
    "- A **10 MeV neutron source** located at `z = -90.0005 cm`, emitting particles along the `+z` axis.\n",
    "- A **cylindrical concrete shield** aligned with the beam axis, with **radius = 100 cm** and **length = 90 cm**.\n",
    "- A longitudinal subdivision into **20 consecutive scoring cells** (`cell_00` ‚Ä¶ `cell_19`):\n",
    "\n",
    "  - `cell_00`: upstream region (closest to the source), **vacuum (G4_Galactic)**  \n",
    "  - `cell_01` ‚Ä¶ `cell_18`: intermediate **concrete** slices  \n",
    "  - `cell_19`: downstream region (deep-penetration ROI), **vacuum (G4_Galactic)**\n",
    "\n",
    "As neutrons propagate through the concrete, they:\n",
    "- Scatter\n",
    "- Lose energy\n",
    "- Are absorbed\n",
    "\n",
    "As a result:\n",
    "- The particle population decreases rapidly with depth\n",
    "- Only a very small fraction of histories reaches the deepest cells\n",
    "\n",
    "This rapid attenuation makes estimators in deep regions (such as `cell_19`)\n",
    "**noisy and slow to converge**.\n",
    "\n",
    "---\n",
    "\n",
    "## Geometry visualization (illustrative)\n",
    "\n",
    "The following figures illustrate the simulated geometry and particle transport.\n",
    "\n",
    "> ‚ö†Ô∏è **Important note**  \n",
    "> The SWAN computing environment does not provide a graphical visualization interface for Geant4.\n",
    "> Therefore, these images are shown **for conceptual understanding only**.\n",
    "> All simulations in this hands-on are executed in **batch mode**.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"https://cernbox.cern.ch/remote.php/dav/public-files/6kS1LLi0qi19UcE/geometry_1_HO1.png\" style=\"width:60%\">\n",
    "  <figcaption>Overall geometry of the deep shielding problem.</figcaption>\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    "  <img src=\"https://cernbox.cern.ch/remote.php/dav/public-files/VesUtjTPNa24rOS/geometry_2_HO1.png\" style=\"width:60%\">\n",
    "  <figcaption>Longitudinal subdivision of the concrete shield into scoring cells.</figcaption>\n",
    "</figure>\n",
    "\n",
    "---\n",
    "\n",
    "## Scoring and physical meaning of the output <a name=\"scoring-and-physical-meaning-of-the-output\"></a>\n",
    "\n",
    "### Scoring quantities\n",
    "\n",
    "For each cell (`cell_00` ‚Ä¶ `cell_19`), the simulation reports several estimators.\n",
    "The most relevant ones for this exercise are:\n",
    "\n",
    "- **Track-length (SL)**  \n",
    "  The total particle track length accumulated in the cell, averaged per primary event.\n",
    "\n",
    "- **Weighted track-length (SLW)**  \n",
    "  The track-length multiplied by the particle statistical weight.  \n",
    "  This is the **unbiased estimator** used when variance reduction techniques are enabled.\n",
    "\n",
    "In this hands-on, the first runs are **fully analog**:\n",
    "- All particle weights are equal to 1\n",
    "- Therefore, `SL` and `SLW` are numerically identical\n",
    "\n",
    "---\n",
    "\n",
    "### Interpreting the per-cell table\n",
    "\n",
    "For each cell, the output table includes:\n",
    "\n",
    "- `Tr.Entering`  \n",
    "  Number of track entries into the cell\n",
    "\n",
    "- `Population`  \n",
    "  A measure of how much particle history lives in that cell\n",
    "\n",
    "- `Av.Tr.WGT`  \n",
    "  Average particle weight (equal to 1 in analog runs)\n",
    "\n",
    "- `SL` and `Sigma(SL)`  \n",
    "  Mean track-length estimator and its statistical uncertainty\n",
    "\n",
    "- `SLW` and `Sigma(SLW)`  \n",
    "  Weight-corrected estimator and its uncertainty\n",
    "\n",
    "---\n",
    "\n",
    "## Region of interest and goal <a name=\"region-of-interest-and-goal\"></a>\n",
    "\n",
    "The **region of interest (ROI)** in this hands-on is:\n",
    "\n",
    "- **`cell_19`**, the deepest cell in the shield\n",
    "\n",
    "Because very few particles reach this region, its track-length estimator\n",
    "is dominated by rare events and exhibits large statistical fluctuations.\n",
    "\n",
    "## Goal of this hands-on\n",
    "\n",
    "Your objective is to estimate the **track-length estimator in `cell_19`**\n",
    "with a **relative statistical uncertainty below 10%**:\n",
    "\n",
    "$$\n",
    "R = \\frac{\\sigma}{\\bar{x}} < 10\\%\n",
    "$$\n",
    "\n",
    "To achieve this, you will:\n",
    "- Run simulations with increasing statistics\n",
    "- Analyze the convergence of the estimator\n",
    "- Measure performance using wall-clock time and the Figure of Merit (FOM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fc05c6-754a-44a8-8130-091da3f7a877",
   "metadata": {},
   "source": [
    "---\n",
    "## Simulation output <a name=\"simulation-output\"></a>\n",
    "\n",
    "Each simulation run produces output in two forms:\n",
    "\n",
    "1. **Screen output** (printed in the notebook cell)\n",
    "2. **Text files written to disk** (saved automatically by the code)\n",
    "\n",
    "You must be able to read and interpret both.\n",
    "\n",
    "---\n",
    "\n",
    "## Screen output\n",
    "\n",
    "### Per-cell results\n",
    "\n",
    "For each scoring cell (`cell_00` ‚Ä¶ `cell_19`), the code prints a table containing:\n",
    "\n",
    "- `Tr.Entering`  \n",
    "- `Population`  \n",
    "- `Av.Tr.WGT`  \n",
    "- `SL` and `Sigma(SL)`  \n",
    "- `SLW` and `Sigma(SLW)`\n",
    "\n",
    "These quantities are reported **per primary event**.\n",
    "\n",
    "In the analog runs used in this hands-on:\n",
    "- All particle weights are equal to 1\n",
    "- Therefore, `SL` and `SLW` are numerically identical\n",
    "\n",
    "---\n",
    "\n",
    "### Region-of-interest summary\n",
    "\n",
    "At the end of each run, a summary block is printed for the **region of interest (`cell_19`)**, including:\n",
    "\n",
    "- `Mean(SLW)`\n",
    "- `StdErr(SLW)`\n",
    "- `Relative error (%)`\n",
    "- `Wall-clock time (s)`\n",
    "- `Figure of Merit (FOM)`\n",
    "\n",
    "The **Figure of Merit** is defined as:\n",
    "\n",
    "$$\n",
    "\\text{FOM} = \\frac{1}{R^2 \\cdot T}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $R$ is the relative statistical error\n",
    "- $T$ is the wall-clock time\n",
    "\n",
    "---\n",
    "\n",
    "## Files written to disk (`build/out/`)\n",
    "\n",
    "In addition to the screen output, each run writes text files to disk.\n",
    "\n",
    "### Per-run cell tables\n",
    "For each run, a file containing the full per-cell scoring tables is written:\n",
    "\n",
    "- `b01_cells_mode{mode}_run{runID}_N{Nevt}.tsv`\n",
    "\n",
    "where:\n",
    "- `{mode}` indicates the simulation mode (`0` for analog simulations and `-1` for biased ones)\n",
    "- `{runID}` is the run identifier (usually `0`)\n",
    "- `{Nevt}` is the number of primary events.\n",
    "\n",
    "So if you run an analog simulation with N=100,000 primary events, your output file would be:\n",
    "\n",
    "- `b01_cells_mode0_run0_N100000.tsv`\n",
    "\n",
    "---\n",
    "\n",
    "### Cumulative summary file\n",
    "\n",
    "A cumulative summary file is also **updated**:\n",
    "\n",
    "- `b01_summary.tsv`\n",
    "\n",
    "This file collects, for each run:\n",
    "- Number of events\n",
    "- Mean estimator value in `cell_19`\n",
    "- Statistical uncertainty\n",
    "- Relative error\n",
    "- Execution time\n",
    "- FOM\n",
    "\n",
    "You will use this file later to compare runs and analyze convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fff26a2-4934-46da-8f70-424956e051ee",
   "metadata": {},
   "source": [
    "## Download and compilation <a name=\"download-and-compilation\"></a>\n",
    "\n",
    "We will now download the simulation code from GitHub and compile it inside SWAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e94b51b-5a8e-4fbe-ba54-b42366ef598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Executing this cell will download and compile the exercise\n",
    "#\n",
    "from time import time\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "t1 = time()\n",
    "\n",
    "# --- Workspace setup ---\n",
    "%cd\n",
    "%mkdir -p CSC26/VR\n",
    "%cd CSC26/VR\n",
    "\n",
    "base_dir = Path.cwd()\n",
    "repo_dir = base_dir / \"hands_on_1\"\n",
    "\n",
    "print(\"Obtaining source code from GitHub...\")\n",
    "\n",
    "# --- Robust clone (safe to re-run the cell) ---\n",
    "if repo_dir.exists():\n",
    "    print(\"Repository already exists. Removing it to ensure a clean setup...\")\n",
    "    shutil.rmtree(repo_dir)\n",
    "\n",
    "!git --no-pager clone https://github.com/jromero-barrientos/hands_on_1.git\n",
    "\n",
    "# --- Build directory ---\n",
    "%mkdir -p hands_on_1/build\n",
    "%cd hands_on_1/build\n",
    "\n",
    "print(\"Running cmake...\")\n",
    "!cmake ..\n",
    "\n",
    "print(\"Running make...\")\n",
    "!make -j\n",
    "\n",
    "# --- Compilation check ---\n",
    "exe = Path(\"exampleB01\")\n",
    "if exe.is_file() and os.access(exe, os.X_OK):\n",
    "    print(f\"‚úÖ Compilation successful: '{exe.name}' found.\")\n",
    "else:\n",
    "    raise RuntimeError(\n",
    "        \"‚ùå Compilation failed: executable 'exampleB01' not found.\\n\"\n",
    "        \"Check the output above.\"\n",
    "    )\n",
    "\n",
    "# --- Optional sanity run ---\n",
    "print(\"Running a short test run (100 events)...\")\n",
    "\n",
    "mode = -1   # analog mode\n",
    "N = 100\n",
    "\n",
    "env = os.environ.copy()\n",
    "env[\"B01_MODE\"] = str(mode)\n",
    "\n",
    "# IMPORTANT: use explicit path to the executable\n",
    "subprocess.run([f\"./{exe.name}\", str(mode), str(N)], env=env, check=True)\n",
    "\n",
    "print(\"‚úÖ Test run completed\")\n",
    "\n",
    "# --- Final info ---\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "t2 = time()\n",
    "print(\"Installed exercise in {:.2f} minutes\".format((t2 - t1) / 60.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636ec7ed-e5a6-4c5f-9f99-14c92e6c11c4",
   "metadata": {},
   "source": [
    "## Compilation and sanity check <a name=\"compilation-and-sanity-check\"></a>\n",
    "\n",
    "If everything worked correctly, you should have observed the following:\n",
    "\n",
    "- The source code was successfully downloaded from GitHub.\n",
    "- The project compiled without errors using **Geant4 11.3.2** (LCG release 108 on SWAN).\n",
    "- An executable called **`exampleB01`** was created inside the `build/` directory.\n",
    "- A short test run with **100 events** was executed successfully.\n",
    "\n",
    "During the test run, you should have seen:\n",
    "- A table printed to screen with one row per cell (`cell_00` ‚Ä¶ `cell_19`).\n",
    "- Columns such as:\n",
    "  - `Tr.Entering`\n",
    "  - `Population`\n",
    "  - `Av.Tr.WGT`\n",
    "  - `SL` and `Sigma(SL)`\n",
    "  - `SLW` and `Sigma(SLW)`\n",
    "- A summary at the end showing the **track-length estimator in `cell_19`**, together with its uncertainty and execution time (unless the mean, sigma or time are zero)\n",
    "\n",
    "üëâ **At this stage, do not worry about the numerical values.**  \n",
    "\n",
    "This run is only meant to verify that:\n",
    "- The code compiles correctly.\n",
    "- The simulation runs as expected.\n",
    "- You are familiar with the structure of the output.\n",
    "\n",
    "In the next steps, we will start running simulations with larger numbers of particles and analyze the results in detail.\n",
    "\n",
    "#### Important: You should now be located in `~/CSC26/VR/hands_on_1/build/.`\n",
    "\n",
    "If you are not sure, run:\n",
    "- `!pwd`\n",
    "\n",
    "If needed, go back to the build directory with:\n",
    "- `%cd ~/CSC26/VR/hands_on_1/build`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355ec2d1-c64d-4987-880d-08877a0ffce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc954125-3130-4d7a-b75f-d80c621373b3",
   "metadata": {},
   "source": [
    "## First analog run <a name=\"first-analog-run\"></a>\n",
    "\n",
    "We now perform the **first analog simulation** with non-trivial statistics.\n",
    "\n",
    "### Run configuration\n",
    "\n",
    "- **Run mode:** analog (no variance reduction)\n",
    "- **Number of events:**  \n",
    "  \n",
    "  N_1 = 100,000\n",
    "  \n",
    "- **All particle weights:** equal to 1\n",
    "\n",
    "This run serves as a **baseline**:\n",
    "\n",
    "- It allows us to understand the structure of the output.\n",
    "- It shows how particle population and estimators evolve with depth.\n",
    "- It provides a first estimate of the track-length in the deep cell (`cell_19`).\n",
    "\n",
    "### What to pay attention to\n",
    "\n",
    "When examining the output table:\n",
    "\n",
    "- How do `Population` and `Tr.Entering` change with cell index?\n",
    "- Compare shallow cells (e.g. `cell_03`) with deep cells (`cell_19`).\n",
    "- Verify that:\n",
    "  - `Av.Tr.WGT = 1`\n",
    "  - `SL ‚â° SLW` (as expected for an analog run)\n",
    "\n",
    "At the end of the output, note the **ROI summary** for `cell_19`:\n",
    "- Mean(SLW)\n",
    "- StdErr(SLW)\n",
    "- Relative error (%)\n",
    "- Run time\n",
    "- FOM\n",
    "\n",
    "You will record these values in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb65f378-d64e-433b-bc7d-68d8bdb8e48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First analog run: N = 100,000 events\n",
    "\n",
    "import os, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "base_dir  = Path.home() / \"CSC26/VR/hands_on_1\"\n",
    "build_dir = base_dir / \"build\"\n",
    "exe = build_dir / \"exampleB01\"  # estando en build/\n",
    "mode = -1\n",
    "N = 100000\n",
    "\n",
    "env = os.environ.copy()\n",
    "env[\"B01_MODE\"] = str(mode)\n",
    "\n",
    "subprocess.run([str(exe), str(mode), str(N)], env=env, check=True)\n",
    "\n",
    "print(\"\\nRun completed.\")\n",
    "print(\"Check the screen output and the files written in build/out/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fcd879-ad74-402b-8da8-60d08a04dfdd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Interpreting the results of the first analog run <a name=\"interpreting-the-results-of-the-first-analog-run\"></a>\n",
    "\n",
    "After running the first analog simulation (`N = 100,000`), carefully inspect:\n",
    "\n",
    "- The **per-cell table** printed on screen.\n",
    "- The **ROI summary for `cell_19`** printed at the end of the run.\n",
    "\n",
    "Answer the following questions **in your own words**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Particle population and depth\n",
    "\n",
    "Look at the columns **`Tr.Entering`** and **`Population`**.\n",
    "\n",
    "- How do these quantities change as we move from `cell_00` to `cell_19`?\n",
    "- What physical processes in a concrete shield explain this behavior?\n",
    "- Why does the population drop so dramatically in the deepest cells?\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Average track weight\n",
    "\n",
    "Examine the column **`Av.Tr.WGT`**.\n",
    "\n",
    "- What value does it take in all cells?\n",
    "- Why is this expected in an **analog** simulation?\n",
    "- What would it physically mean if this value were different from 1?\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Track-length estimators: SL vs SLW\n",
    "\n",
    "Compare **`SL`** and **`SLW`** for all cells.\n",
    "\n",
    "- Are they numerically identical?\n",
    "- Why does this happen in this run?\n",
    "- Which estimator would remain valid if particle weights were not equal to 1?\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Statistical uncertainties and depth\n",
    "\n",
    "Focus on **`Sigma(SL)`** and **`Sigma(SLW)`**.\n",
    "\n",
    "- How do the uncertainties change as a function of cell index?\n",
    "- Compare the relative error in a shallow cell (e.g. `cell_03`) with that in the deepest cell (`cell_19`).\n",
    "- Why is the relative error in `cell_19` still large despite running 100,000 events?\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Region of Interest (cell_19): cost vs precision\n",
    "\n",
    "Now look at the **ROI summary for `cell_19`**, printed at the end of the run.\n",
    "\n",
    "- Write down:\n",
    "  - `Mean(SLW)`\n",
    "  - `StdErr(SLW)`\n",
    "  - **Relative error (%)**\n",
    "  - **Execution time (s)**\n",
    "  - **FOM (1/s)**\n",
    "\n",
    "- Is the relative uncertainty below the target **10%**?\n",
    "- How much computational time was required to reach this precision?\n",
    "\n",
    "---\n",
    "\n",
    "### 6. First conclusions\n",
    "\n",
    "Based on this first run only:\n",
    "\n",
    "- Is the statistical precision in `cell_19` acceptable for our goal?\n",
    "- What do you expect would happen to:\n",
    "  - the uncertainty,\n",
    "  - the execution time,\n",
    "if the number of events were increased?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65989202-f851-427d-bb9b-f664a539ef30",
   "metadata": {},
   "source": [
    "## Second analog run: increasing statistics <a name=\"second_analog_run_increasing_statistics\"></a>\n",
    "\n",
    "The first run with **100,000 events** showed that the statistical uncertainty\n",
    "in the deepest cell (`cell_19`) is still very large.\n",
    "\n",
    "To reduce the relative uncertainty, we now increase the number of events\n",
    "by **one full order of magnitude**.\n",
    "\n",
    "In the next run you will execute an **analog simulation with**:\n",
    "\n",
    "- **N = 1,000,000 events**\n",
    "\n",
    "After this run, you will:\n",
    "- Repeat the same analysis as before.\n",
    "- Compare the results with the 100k run.\n",
    "- Pay special attention to:\n",
    "  - `cell_03` (shallow region),\n",
    "  - `cell_19` (deep penetration region),\n",
    "  - execution time and FOM.\n",
    "\n",
    "Do **not** change any simulation parameters other than `N`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22c2ed4-8e97-40f4-83ee-5938ffdee85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second analog run: N = 1,000,000 events\n",
    "\n",
    "import os, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "base_dir  = Path.home() / \"CSC26/VR/hands_on_1\"\n",
    "build_dir = base_dir / \"build\"\n",
    "exe = build_dir / \"exampleB01\"  # estando en build/\n",
    "mode = -1\n",
    "N = 1000000\n",
    "\n",
    "env = os.environ.copy()\n",
    "env[\"B01_MODE\"] = str(mode)\n",
    "\n",
    "subprocess.run([str(exe), str(mode), str(N)], env=env, check=True)\n",
    "\n",
    "print(\"\\nRun completed.\")\n",
    "print(\"Check the screen output and the files written in build/out/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a549d387-0210-46be-860d-c6bde78c9c64",
   "metadata": {},
   "source": [
    "## Comparing analog runs: 100k vs 1,000,000 events <a name=\"comparing_analog_runs\"></a>\n",
    "\n",
    "You have now executed **two analog simulations** of the *same physical problem*:\n",
    "\n",
    "- Run 1: **N = 100,000**\n",
    "- Run 2: **N = 1,000,000**\n",
    "\n",
    "The **only difference** between the two runs is the number of particle histories.\n",
    "\n",
    "This comparison allows you to study:\n",
    "- Statistical convergence\n",
    "- Cost versus precision\n",
    "- The practical limits of brute-force Monte Carlo\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Shallow regions (where statistics are abundant)\n",
    "\n",
    "Focus on a shallow cell, for example **`cell_03`**, where many particle histories contribute.\n",
    "\n",
    "Compare between the two runs:\n",
    "- `SL` and `SLW`\n",
    "- `Sigma(SL)` and `Sigma(SLW)`\n",
    "- Their respective relative errors\n",
    "\n",
    "**Questions:**\n",
    "- Does the estimator value change noticeably?\n",
    "- Does the statistical uncertainty decrease when increasing N by a factor 10?\n",
    "- Is this behavior consistent with what you expect from Monte Carlo statistics?\n",
    "\n",
    "What does this tell you about estimator behavior in regions with **good statistics**?\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Deep penetration region (where statistics are scarce)\n",
    "\n",
    "Now focus on the **region of interest**, **`cell_19`**.\n",
    "\n",
    "Compare between the two runs:\n",
    "- `Mean(SLW)` and `StdErr(SLW)`\n",
    "- Relative error (%)\n",
    "- Execution time and FOM\n",
    "\n",
    "**Questions:**\n",
    "- Did the precision improve when increasing N by a factor 10?\n",
    "- By how much did the relative error decrease?\n",
    "- Are you anywhere near the **10%** target?\n",
    "- Based on this comparison, does ‚Äújust increasing N‚Äù look like a practical strategy?\n",
    "\n",
    "---\n",
    "\n",
    "### Interim conclusion\n",
    "\n",
    "From these two runs, you should already observe a clear contrast:\n",
    "\n",
    "- In shallow regions, increasing N behaves exactly as expected.\n",
    "- In deep regions, convergence is **much slower** and increasingly expensive.\n",
    "\n",
    "---\n",
    "\n",
    "‚û°Ô∏è **Next step**\n",
    "\n",
    "You will now be given the results of a much larger **analog run**\n",
    "(**10 million events**) executed on the same SWAN environment.\n",
    "\n",
    "Using that run, you will:\n",
    "- Quantify how uncertainty scales with N\n",
    "- Estimate the computational cost required to reach tighter targets\n",
    "  (e.g. **10%** and **1%** relative uncertainty)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d006c71-7193-4afe-a191-7fc85e7dda71",
   "metadata": {},
   "source": [
    "## Importing a precomputed high-statistics analog run (10 million events)\n",
    "\n",
    "We will now work with the results of an **analog simulation with 10,000,000 events**\n",
    "that has been **precomputed for you**.\n",
    "\n",
    "### Important: this is exactly the same simulation\n",
    "\n",
    "This run is identical to the ones you have already executed:\n",
    "\n",
    "- Same geometry (20 longitudinal scoring cells)\n",
    "- Same materials\n",
    "- Same physics list (**FTFP_BERT_HP**)\n",
    "- Same scoring quantities\n",
    "- Same output format\n",
    "- Same execution environment (SWAN / LCG 108 / Geant4 11.3.2)\n",
    "\n",
    "‚úÖ **The only difference is the number of events:**\n",
    "simwv\n",
    "$$\n",
    "N = 10{,}000{,}000\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Why are we importing this run instead of running it ourselves?\n",
    "\n",
    "Simply because of **execution time**.\n",
    "\n",
    "Running 10 million events:\n",
    "- Takes **several minutes** on SWAN\n",
    "- Would consume a large fraction of the hands-on session\n",
    "- Would lead to idle waiting time rather than analysis and discussion\n",
    "\n",
    "Instead, we provide this run so that you can:\n",
    "- Inspect realistic high-statistics results\n",
    "- Use them as a **reference point** for scaling arguments\n",
    "- Quantify the true cost of brute-force Monte Carlo\n",
    "\n",
    "üëâ When you load the file, **pay close attention to the wall-clock time**\n",
    "reported for this 10M run and compare it with your previous runs.\n",
    "\n",
    "**Question (think before answering):**  \n",
    "Based on that time, would it be realistic for everyone to run 10M events\n",
    "during this hands-on session?\n",
    "\n",
    "---\n",
    "\n",
    "### What we will do next\n",
    "\n",
    "In the next code cell, we will:\n",
    "\n",
    "- Load your own analog summary file (e.g. from the 100k or 1M run), if available\n",
    "- Load the precomputed **10M analog summary and per-cell table**\n",
    "- Use these results **for analysis only**\n",
    "  - We will **not modify** your original output files\n",
    "\n",
    "These data will allow us to:\n",
    "- Study how uncertainty scales with the number of events\n",
    "- Estimate the computational cost required to reach specific precision targets\n",
    "- Decide whether brute-force analog simulation is a viable strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64808e44-c3ad-486b-8d8d-8ca66f22fe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Base folders (robust: no $HOME string issues)\n",
    "base_dir  = Path.home() / \"CSC26/VR/hands_on_1\"\n",
    "build_dir = base_dir / \"build\"\n",
    "out_dir   = build_dir / \"out\"\n",
    "pre_dir   = base_dir / \"pcruns\"\n",
    "\n",
    "# --- Files\n",
    "student_summary  = out_dir / \"b01_summary.tsv\"\n",
    "precomp_summary  = pre_dir / \"b01_summary_10M.tsv\"\n",
    "precomp_cells10m = pre_dir / \"b01_cells_mode-1_run0_N10000000.tsv\"\n",
    "\n",
    "print(\"=== Paths ===\")\n",
    "print(\"base_dir        :\", base_dir)\n",
    "print(\"build_dir       :\", build_dir)\n",
    "print(\"out_dir         :\", out_dir)\n",
    "print(\"pre_dir         :\", pre_dir)\n",
    "print(\"student_summary :\", student_summary)\n",
    "print(\"precomp_summary :\", precomp_summary)\n",
    "print(\"precomp_cells10m:\", precomp_cells10m)\n",
    "\n",
    "# --- Load student's summary (if any)\n",
    "if student_summary.exists():\n",
    "    df_student = pd.read_csv(student_summary, sep=\"\\t\", comment=\"#\")\n",
    "    print(f\"\\nLoaded STUDENT summary: {student_summary}  ({len(df_student)} runs)\")\n",
    "else:\n",
    "    df_student = pd.DataFrame()\n",
    "    print(\"\\nNo student summary found yet (b01_summary.tsv).\")\n",
    "\n",
    "# --- Load precomputed 10M summary\n",
    "if not precomp_summary.exists():\n",
    "    raise FileNotFoundError(f\"Missing precomputed summary: {precomp_summary}\")\n",
    "df_10m = pd.read_csv(precomp_summary, sep=\"\\t\", comment=\"#\")\n",
    "print(f\"Loaded PRECOMPUTED summary: {precomp_summary}  ({len(df_10m)} run)\")\n",
    "\n",
    "# --- Combine in-memory (do NOT overwrite student file)\n",
    "df_all = pd.concat([df_student, df_10m], ignore_index=True)\n",
    "\n",
    "print(\"\\n=== Combined summary (tail) ===\")\n",
    "display(df_all.tail(10))\n",
    "\n",
    "# Optional: write a combined file for analysis only\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "combined_path = out_dir / \"b01_summary_with_precomputed.tsv\"\n",
    "df_all.to_csv(combined_path, sep=\"\\t\", index=False)\n",
    "print(f\"\\nWrote combined summary (analysis-only): {combined_path}\")\n",
    "\n",
    "# --- Load precomputed 10M per-cell table (so they can inspect it)\n",
    "if not precomp_cells10m.exists():\n",
    "    raise FileNotFoundError(f\"Missing precomputed 10M cell table: {precomp_cells10m}\")\n",
    "\n",
    "df_cells10m = pd.read_csv(precomp_cells10m, sep=\"\\t\", comment=\"#\")\n",
    "print(f\"\\nLoaded PRECOMPUTED 10M cell table: {precomp_cells10m}  ({len(df_cells10m)} rows)\")\n",
    "\n",
    "print(\"\\n=== 10M cell table (head) ===\")\n",
    "display(df_cells10m)\n",
    "\n",
    "print(\"\\n=== 10M cell table: ROI row (cell 19) ===\")\n",
    "display(df_cells10m[df_cells10m[\"cell\"] == 19])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0613be0c-cac1-4979-8126-97dc302da28e",
   "metadata": {},
   "source": [
    "## Diagnostics plots (Analog): scaling behavior\n",
    "\n",
    "In the lectures we discussed two key scaling behaviors expected in\n",
    "a standard **analog Monte Carlo simulation**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1) Statistical scaling\n",
    "\n",
    "For a fixed physical problem, the relative statistical uncertainty\n",
    "\n",
    "$$\n",
    "R \\equiv \\frac{\\sigma}{\\mu}\n",
    "$$\n",
    "\n",
    "is expected to scale approximately as:\n",
    "\n",
    "$$\n",
    "R \\propto \\frac{1}{\\sqrt{N}}\n",
    "$$\n",
    "\n",
    "This means that:\n",
    "- Reducing the uncertainty by a factor of 2\n",
    "- Requires increasing the number of events by a factor of 4\n",
    "\n",
    "We will check this by plotting:\n",
    "\n",
    "- $R$ vs. $1/\\sqrt{N}$\n",
    "\n",
    "A roughly linear trend indicates that the Monte Carlo estimator behaves\n",
    "as expected from basic statistical theory.\n",
    "\n",
    "---\n",
    "\n",
    "### 2) Runtime scaling\n",
    "\n",
    "For an analog simulation of fixed complexity:\n",
    "\n",
    "- The wall-clock runtime $T$ is expected to scale approximately linearly with $N$:\n",
    "\n",
    "$$\n",
    "T \\propto N\n",
    "$$\n",
    "\n",
    "We will check this by plotting:\n",
    "\n",
    "- $T$ vs. $N$\n",
    "\n",
    "This plot helps quantify the **computational cost** of improving statistical precision\n",
    "by brute-force increase of the number of events.\n",
    "\n",
    "---\n",
    "\n",
    "### What these diagnostics tell us\n",
    "\n",
    "These plots are **diagnostic tools**, not strict proofs.\n",
    "\n",
    "They allow us to:\n",
    "- Verify that the Monte Carlo simulation behaves sensibly\n",
    "- Estimate how costly it is to reduce the statistical uncertainty\n",
    "- Anticipate whether increasing $N$ alone is a practical strategy\n",
    "\n",
    "In the next steps, we will use these trends to estimate the cost\n",
    "of reaching much tighter uncertainties in deep shielding problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648b304a-adf9-4047-925e-f92fae12b0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths you defined earlier (keeping your naming)\n",
    "build_dir = Path.home() / \"CSC26/VR/hands_on_1/build\"\n",
    "out_dir   = build_dir / \"out\"\n",
    "pre_dir   = Path.home() / \"CSC26/VR/hands_on_1/pcruns\"\n",
    "\n",
    "student_summary  = out_dir / \"b01_summary.tsv\"\n",
    "precomp_summary  = pre_dir / \"b01_summary_10M.tsv\"\n",
    "\n",
    "def read_summary_tsv(path: Path) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing summary file: {path}\")\n",
    "    df = pd.read_csv(path, sep=\"\\t\")\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "# Load and merge (student runs + precomputed 10M)\n",
    "dfs = []\n",
    "for p in [student_summary, precomp_summary]:\n",
    "    try:\n",
    "        dfs.append(read_summary_tsv(p))\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Could not read {p}: {e}\")\n",
    "\n",
    "if len(dfs) == 0:\n",
    "    raise RuntimeError(\"No summary files could be loaded.\")\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(\"Columns found:\", list(df.columns))\n",
    "\n",
    "# Expected columns (from our RunAction design):\n",
    "col_N    = \"Nevt\"\n",
    "col_T    = \"time_real_s\"\n",
    "col_R    = \"roi_relerr\"\n",
    "col_FOM  = \"FOM_1_per_s\"\n",
    "\n",
    "missing = [c for c in [col_N, col_T, col_R] if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required columns in summary: {missing}\")\n",
    "\n",
    "# Convert key columns to numeric (robust to strings)\n",
    "for c in [col_N, col_T, col_R]:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "if col_FOM in df.columns:\n",
    "    df[col_FOM] = pd.to_numeric(df[col_FOM], errors=\"coerce\")\n",
    "\n",
    "# Keep only analog runs if 'mode' exists\n",
    "if \"mode\" in df.columns:\n",
    "    df_analog = df[df[\"mode\"].astype(str).str.strip() == \"-1\"].copy()\n",
    "else:\n",
    "    df_analog = df.copy()\n",
    "\n",
    "# Compute FOM if not present (or if present but you want to override, comment next if-block and always compute)\n",
    "if col_FOM not in df_analog.columns:\n",
    "    df_analog[col_FOM] = 1.0 / (df_analog[col_R]**2 * df_analog[col_T])\n",
    "\n",
    "# Clean up FOM: set non-physical/undefined values to NaN\n",
    "# (e.g., R<=0, T<=0, or division artifacts)\n",
    "bad_fom = (~np.isfinite(df_analog[col_FOM])) | (df_analog[col_FOM] <= 0) | (df_analog[col_R] <= 0) | (df_analog[col_T] <= 0)\n",
    "df_analog.loc[bad_fom, col_FOM] = np.nan\n",
    "\n",
    "# Sort by N\n",
    "df_analog = df_analog.sort_values(col_N)\n",
    "\n",
    "# ---- Plot 1: R vs 1/sqrt(N) ----\n",
    "mask_R = (\n",
    "    np.isfinite(df_analog[col_N]) & (df_analog[col_N] > 0) &\n",
    "    np.isfinite(df_analog[col_R]) & (df_analog[col_R] > 0)\n",
    ")\n",
    "df_R = df_analog.loc[mask_R].copy()\n",
    "\n",
    "N_R = df_R[col_N].to_numpy(dtype=float)\n",
    "R   = df_R[col_R].to_numpy(dtype=float)\n",
    "x   = 1.0 / np.sqrt(N_R)\n",
    "\n",
    "if len(x) >= 2:\n",
    "    m, b = np.polyfit(x, R, 1)\n",
    "    print(f\"Fit: R ‚âà {m:.3g}*(1/sqrt(N)) + {b:.3g}\")\n",
    "else:\n",
    "    m, b = np.nan, np.nan\n",
    "    print(\"[WARN] Not enough valid points for a linear fit of R vs 1/sqrt(N).\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, R, \"o\", label=\"Runs\")\n",
    "if np.isfinite(m) and np.isfinite(b):\n",
    "    xx = np.linspace(x.min()*0.9, x.max()*1.1, 200)\n",
    "    plt.plot(xx, m*xx + b, \"-\", label=\"Linear fit\")\n",
    "plt.xlabel(\"1/sqrt(N)\")\n",
    "plt.ylabel(\"Relative uncertainty R = sigma/mu (cell_19)\")\n",
    "plt.title(\"Analog scaling check: R vs 1/sqrt(N)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ---- Plot 2: T vs N ----\n",
    "mask_T = (\n",
    "    np.isfinite(df_analog[col_N]) & (df_analog[col_N] > 0) &\n",
    "    np.isfinite(df_analog[col_T]) & (df_analog[col_T] > 0)\n",
    ")\n",
    "df_T = df_analog.loc[mask_T].copy()\n",
    "\n",
    "N_T = df_T[col_N].to_numpy(dtype=float)\n",
    "T   = df_T[col_T].to_numpy(dtype=float)\n",
    "\n",
    "if len(N_T) >= 2:\n",
    "    mT, bT = np.polyfit(N_T, T, 1)\n",
    "    print(f\"Fit: T ‚âà {mT:.3e}*N + {bT:.3g} (seconds)\")\n",
    "else:\n",
    "    mT, bT = np.nan, np.nan\n",
    "    print(\"[WARN] Not enough valid points for a linear fit of T vs N.\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(N_T, T, \"o\", label=\"Runs\")\n",
    "if np.isfinite(mT) and np.isfinite(bT):\n",
    "    NN = np.linspace(N_T.min()*0.9, N_T.max()*1.1, 200)\n",
    "    plt.plot(NN, mT*NN + bT, \"-\", label=\"Linear fit\")\n",
    "plt.xlabel(\"N events\")\n",
    "plt.ylabel(\"Wall time T (s)\")\n",
    "plt.title(\"Runtime scaling check: T vs N\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Optional: Show a compact table (including NaNs so students can see what's undefined)\n",
    "out_cols = [c for c in [\"mode\", col_N, col_T, col_R] if c in df_analog.columns]\n",
    "out = df_analog[out_cols].copy()\n",
    "out[\"R_%\"] = 100.0*out[col_R]\n",
    "print(\"\\nSummary (analog runs):\")\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea60861f-5887-4438-8bf9-7b0d364bf87f",
   "metadata": {},
   "source": [
    "## Diagnostics (analog runs): scaling of uncertainty and runtime\n",
    "\n",
    "You have just produced two diagnostic plots for **analog** runs in the ROI (`cell_19`):\n",
    "\n",
    "1) **Relative uncertainty** $R$ vs $1/\\sqrt{N}$  (statistical scaling check)  \n",
    "2) **Wall time** $T$ vs $N$ (runtime scaling check)\n",
    "\n",
    "These are *sanity diagnostics*: they help us see whether the simulation behaves like a standard Monte Carlo estimator and whether brute-force scaling is a plausible strategy.\n",
    "\n",
    "---\n",
    "\n",
    "### 1) Statistical scaling in the ROI: does $R$ decrease with $N$?\n",
    "\n",
    "- As $N$ increases, does $R$ decrease?  \n",
    "- Does the plot of $R$ vs $1/\\sqrt{N}$ look **roughly linear** (same trend, not a perfect line)?\n",
    "\n",
    "**Quantitative check (order-of-magnitude, not exact):**  \n",
    "If $N$ increases by a factor of 10, the *ideal* Monte Carlo expectation is:\n",
    "\n",
    "$$\n",
    "R \\propto \\frac{1}{\\sqrt{N}}\n",
    "\\quad\\Rightarrow\\quad\n",
    "\\frac{R(N)}{R(10N)} \\approx \\sqrt{10} \\approx 3.16\n",
    "$$\n",
    "\n",
    "In deep shielding, you should not expect the factor to match $3.16$ perfectly, because the ROI is dominated by **rare histories** at moderate $N$.  \n",
    "So here we only ask:\n",
    "\n",
    "- Is the reduction **clearly of the right scale** (e.g. a factor $\\sim 2$‚Äì$4$)?  \n",
    "- Does increasing $N$ keep pushing $R$ down in a consistent way?\n",
    "\n",
    "Write one sentence summarizing what you observe.\n",
    "\n",
    "---\n",
    "\n",
    "### 2) Runtime scaling: does $T$ increase roughly proportionally to $N$?\n",
    "\n",
    "- Does the wall time increase as you increase $N$?  \n",
    "- Does $T$ vs $N$ look **approximately linear** over the runs you have?\n",
    "\n",
    "A practical check is to compare time ratios:\n",
    "\n",
    "- If $N$ increases by $10\\times$, is $T$ also of the same order ($\\sim 10\\times$)?\n",
    "\n",
    "**About the line fit:**  \n",
    "You do not need a strict criterion. We simply want to see whether the fitted line follows the data trend (no obvious curvature or dramatic departures).\n",
    "\n",
    "---\n",
    "\n",
    "### 3) Practical conclusion: cost vs precision\n",
    "\n",
    "Using both plots together, answer:\n",
    "\n",
    "- Does ‚Äújust increase $N$‚Äù look practical for reaching **$R < 10\\%$** in `cell_19` within a hands-on session?\n",
    "- What is the main limitation: time, or slow uncertainty reduction?\n",
    "\n",
    "Write a short conclusion (2‚Äì4 lines)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eae2281-929e-4ca8-8039-5c72a4053f4f",
   "metadata": {},
   "source": [
    "---\n",
    "## From scaling trends to estimator reliability (near vs ROI)\n",
    "\n",
    "The diagnostics you just did tell us something important:\n",
    "\n",
    "- In the **ROI (`cell_19`)**, increasing $N$ tends to reduce $R$, and runtime tends to grow with $N$.\n",
    "- This is consistent with the expected *Monte Carlo scaling behavior*.\n",
    "\n",
    "However, in deep shielding there is one extra complication:\n",
    "\n",
    "> Even if $R$ decreases with $N$, the **mean estimate** in a rare-event region can still change noticeably at moderate statistics,\n",
    "because new rare contributions start to appear only when $N$ becomes large enough.\n",
    "\n",
    "So we now ask a more practical reliability question:\n",
    "\n",
    "> **Which parts of the geometry look ‚Äúsettled‚Äù already, and which parts still fluctuate strongly with $N$?**\n",
    "\n",
    "To answer that, we compare:\n",
    "- a **near-source cell** with abundant statistics (`cell_03`), and\n",
    "- the **deep ROI** (`cell_19`), dominated by rare histories.\n",
    "\n",
    "This comparison will help us interpret the ROI results and motivate why variance reduction is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b26df1e-9775-4274-b77c-d223fc1f48be",
   "metadata": {},
   "source": [
    "---\n",
    "## Stability check: a near cell vs the deep-penetration ROI\n",
    "\n",
    "Scaling laws describe how uncertainty behaves *on average*.  \n",
    "They do **not** tell us whether the **mean estimate** is already ‚Äúsettled‚Äù at the statistics we can afford.\n",
    "\n",
    "In deep shielding:\n",
    "\n",
    "- **`cell_03` (near-source)** receives many contributions.  \n",
    "  As $N$ increases, the mean should change little, while uncertainty shrinks.\n",
    "\n",
    "- **`cell_19` (ROI)** is dominated by rare histories.  \n",
    "  At moderate $N$, the mean can shift noticeably as new rare contributions start to appear.\n",
    "\n",
    "This is not a bug ‚Äî it is exactly the motivation for variance reduction.\n",
    "\n",
    "---\n",
    "\n",
    "### Task\n",
    "\n",
    "Using the per-run cell tables (`b01_cells_*.tsv`), compare `cell_03` and `cell_19`\n",
    "across the analog runs you have available (e.g. $10^5$, $10^6$, precomputed $10^7$).\n",
    "\n",
    "---\n",
    "\n",
    "### Questions (short answers)\n",
    "\n",
    "1. Does **Mean(SLW)** in `cell_03` change significantly as $N$ increases?  \n",
    "   What does this tell you about estimator stability when statistics are abundant?\n",
    "\n",
    "2. Does **Mean(SLW)** in `cell_19` change significantly as $N$ increases?  \n",
    "   Why is this expected for rare-event regions?\n",
    "\n",
    "3. Compare the **relative uncertainty** in `cell_03` and `cell_19`.  \n",
    "   What physical and statistical effects explain the difference?\n",
    "\n",
    "4. Which cell would you trust more as a **reference region** when checking consistency across different sampling strategies later (analog vs VR)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fac9956-759c-4fac-85f8-cdbcf7913e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Paths (as defined earlier in your notebook) ---\n",
    "build_dir = Path.home() / \"CSC26/VR/hands_on_1/build\"\n",
    "out_dir   = build_dir / \"out\"\n",
    "pre_dir   = Path.home() / \"CSC26/VR/hands_on_1/pcruns\"\n",
    "\n",
    "# Helper: find the per-run tables (student runs + precomputed 10M)\n",
    "student_tables = sorted(out_dir.glob(\"b01_cells_mode-1_run*_N*.tsv\"))\n",
    "precomp_tables = sorted(pre_dir.glob(\"b01_cells_mode-1_run*_N*.tsv\"))  # include 10M table here\n",
    "\n",
    "tables = student_tables + precomp_tables\n",
    "\n",
    "if len(tables) == 0:\n",
    "    raise RuntimeError(\"No per-run cell tables found (b01_cells_mode-1_run*_N*.tsv).\")\n",
    "\n",
    "print(\"Per-run tables found:\")\n",
    "for p in tables:\n",
    "    print(\" -\", p)\n",
    "\n",
    "def parse_N_from_filename(path: Path) -> int:\n",
    "    # expected pattern: ..._N100000.tsv\n",
    "    s = path.name\n",
    "    i = s.rfind(\"_N\")\n",
    "    j = s.rfind(\".tsv\")\n",
    "    return int(s[i+2:j])\n",
    "\n",
    "def load_cells_table(path: Path) -> pd.DataFrame:\n",
    "    # File has comment lines starting with '#'\n",
    "    df = pd.read_csv(path, sep=\"\\t\", comment=\"#\")\n",
    "    # normalize column names (robust to small header edits)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "# Choose cells for stability comparison\n",
    "CELL_NEAR = \"cell_03\"\n",
    "CELL_ROI  = \"cell_19\"\n",
    "\n",
    "rows = []\n",
    "for p in tables:\n",
    "    N = parse_N_from_filename(p)\n",
    "    df = load_cells_table(p)\n",
    "\n",
    "    # Expect columns similar to: cellName, SLW_mean_mm, SLW_stderr_mm, etc.\n",
    "    # Your RunAction writes:\n",
    "    # \"cell  cellName  TrEntering Population AvTrWgt SL_mean_mm SL_stderr_mm SLW_mean_mm SLW_stderr_mm\"\n",
    "    if \"cellName\" not in df.columns:\n",
    "        raise ValueError(f\"Missing 'cellName' column in {p}\")\n",
    "\n",
    "    for cname in [CELL_NEAR, CELL_ROI]:\n",
    "        sub = df[df[\"cellName\"] == cname]\n",
    "        if len(sub) != 1:\n",
    "            raise ValueError(f\"Could not find exactly one row for {cname} in {p}\")\n",
    "        r = sub.iloc[0]\n",
    "\n",
    "        mean = float(r[\"SLW_mean_mm\"])\n",
    "        se   = float(r[\"SLW_stderr_mm\"])\n",
    "        rel  = (se/mean) if mean > 0 else np.nan\n",
    "\n",
    "        rows.append({\n",
    "            \"Nevt\": N,\n",
    "            \"cell\": cname,\n",
    "            \"SLW_mean_mm\": mean,\n",
    "            \"SLW_stderr_mm\": se,\n",
    "            \"rel_uncertainty\": rel,\n",
    "        })\n",
    "\n",
    "stab = pd.DataFrame(rows).sort_values([\"cell\", \"Nevt\"]).reset_index(drop=True)\n",
    "\n",
    "# Print a compact stability table\n",
    "stab_show = stab.copy()\n",
    "stab_show[\"rel_%\"] = 100.0 * stab_show[\"rel_uncertainty\"]\n",
    "print(\"\\nStability comparison (SLW):\")\n",
    "display(stab_show[[\"cell\", \"Nevt\", \"SLW_mean_mm\", \"SLW_stderr_mm\", \"rel_%\"]])\n",
    "\n",
    "# Optional: quantify \"stability\" as fractional change of the mean vs the previous N\n",
    "stab[\"frac_change_vs_prev\"] = np.nan\n",
    "for cell in [CELL_NEAR, CELL_ROI]:\n",
    "    m = stab.loc[stab[\"cell\"] == cell, \"SLW_mean_mm\"].to_numpy()\n",
    "    n = stab.loc[stab[\"cell\"] == cell, \"Nevt\"].to_numpy()\n",
    "    # fractional change: |m_i - m_{i-1}| / m_{i-1}\n",
    "    fc = np.full_like(m, np.nan, dtype=float)\n",
    "    for i in range(1, len(m)):\n",
    "        if m[i-1] != 0:\n",
    "            fc[i] = abs(m[i] - m[i-1]) / abs(m[i-1])\n",
    "    stab.loc[stab[\"cell\"] == cell, \"frac_change_vs_prev\"] = fc\n",
    "\n",
    "stab_fc = stab.copy()\n",
    "stab_fc[\"frac_change_%\"] = 100.0 * stab_fc[\"frac_change_vs_prev\"]\n",
    "print(\"\\nFractional change of Mean(SLW) vs previous N (diagnostic for stability):\")\n",
    "display(stab_fc[[\"cell\", \"Nevt\", \"SLW_mean_mm\", \"frac_change_%\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faf5260-87dc-4153-8cd3-5ec026e5b1b4",
   "metadata": {},
   "source": [
    "## How far can we go with analog simulation?\n",
    "\n",
    "So far, you have observed (for the ROI `cell_19`) that:\n",
    "\n",
    "- The relative uncertainty decreases roughly like\n",
    "  $$\n",
    "  R \\propto \\frac{1}{\\sqrt{N}}\n",
    "  $$\n",
    "- The wall time increases approximately linearly with the number of events,\n",
    "  $$\n",
    "  T \\propto N\n",
    "  $$\n",
    "\n",
    "We now ask a practical question:\n",
    "\n",
    "> **If we keep increasing $N$, how expensive is it to reach the target precision in `cell_19`?**\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1 ‚Äî Organize your runs (no plotting)\n",
    "\n",
    "Fill a small table using the numbers printed in the ROI summary:\n",
    "\n",
    "- $N$\n",
    "- $R$ (relative uncertainty) in `cell_19`\n",
    "- $T$ (wall time)\n",
    "\n",
    "You should have at least:\n",
    "- your runs (e.g. $10^5$, $10^6$)\n",
    "- plus the precomputed analog run (e.g. $10^7$)\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2 ‚Äî Pick one reference run\n",
    "\n",
    "Choose **one** run as reference $(N_0, R_0, T_0)$.\n",
    "\n",
    "**Recommendation:** use the run with the **largest $N$** available (it is usually the most reliable).\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3 ‚Äî Estimate $N$ needed to reach the target\n",
    "\n",
    "Assuming the ideal Monte Carlo scaling holds in the ROI,\n",
    "$$\n",
    "R \\approx R_0 \\sqrt{\\frac{N_0}{N}},\n",
    "$$\n",
    "then the statistics required to reach a target $R_\\text{target}$ are:\n",
    "$$\n",
    "\\frac{N_\\text{target}}{N_0} \\approx \\left(\\frac{R_0}{R_\\text{target}}\\right)^2.\n",
    "$$\n",
    "\n",
    "Compute (order of magnitude is enough):\n",
    "- $N_{10\\%}$ for $R_\\text{target}=10\\%$.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4 ‚Äî Estimate the runtime cost\n",
    "\n",
    "Assuming runtime is roughly linear in $N$,\n",
    "$$\n",
    "T_\\text{target} \\approx T_0 \\frac{N_\\text{target}}{N_0}.\n",
    "$$\n",
    "\n",
    "Estimate:\n",
    "- $T_{10\\%}$ (seconds or minutes)\n",
    "\n",
    "---\n",
    "\n",
    "### Quick check\n",
    "\n",
    "Is reaching $R<10\\%$ by brute-force analog simulation:\n",
    "\n",
    "- feasible within a hands-on session?\n",
    "- feasible as a routine strategy?\n",
    "\n",
    "Explain in **one sentence**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37437ea2-d0ba-45db-bcfa-15fe6b3dd3f8",
   "metadata": {},
   "source": [
    "## Switching on Variance Reduction: Importance Biasing (`mode = 0`)\n",
    "\n",
    "So far we ran **analog Monte Carlo** (`mode = -1`).  \n",
    "We saw that in the **region of interest (ROI)**, **`cell_19`**, the relative uncertainty decreases too slowly with \\(N\\), so reaching the **10% goal** by brute force is impractical in a hands-on setting.\n",
    "\n",
    "Now we will activate **variance reduction** using **importance biasing**.\n",
    "\n",
    "---\n",
    "\n",
    "### What changes (and what does NOT)\n",
    "\n",
    "‚úÖ **Geometry stays exactly the same**  \n",
    "Same world, same cylindrical shielding setup, same segmentation into `cell_00` ‚Ä¶ `cell_19`.\n",
    "\n",
    "‚úÖ **Materials stay exactly the same**  \n",
    "Same concrete shield (and the same non-shield regions as defined in the exercise).\n",
    "\n",
    "‚úÖ **Physics stays exactly the same**  \n",
    "Same physics list: **`FTFP_BERT_HP`** (same processes, same cuts).\n",
    "\n",
    "‚úÖ **Scoring stays exactly the same**  \n",
    "Same estimators and the same output table + ROI summary for `cell_19`.\n",
    "\n",
    "‚ùó The only change is the **transport strategy**: we guide particle histories using **geometric importances**.\n",
    "\n",
    "---\n",
    "\n",
    "### The importance map used in this exercise\n",
    "\n",
    "Inside the shield, importances increase **by a factor of 2 per cell**:\n",
    "\n",
    "$$\n",
    "\\frac{I_{i+1}}{I_i} = 2\n",
    "\\quad\\Rightarrow\\quad\n",
    "I_i = 2^{(i-1)} \\;\\;\\text{for}\\;\\; i=1,2,\\dots,18\n",
    "$$\n",
    "\n",
    "So each step deeper is ‚Äútwice as important‚Äù as the previous one.\n",
    "\n",
    "You will actually *see* the assigned importances printed in the output when `mode = 0`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bf6296-0821-44fb-8c63-e7d42bb65c5e",
   "metadata": {},
   "source": [
    "### Where does the importance map come from?\n",
    "\n",
    "In this exercise, the importance values are defined directly in the Geant4 example source code:\n",
    "\n",
    "- `B01DetectorConstruction.cc`\n",
    "- method: `CreateImportanceStore()`\n",
    "\n",
    "**What it does:**\n",
    "- The code iterates over the longitudinal scoring volumes (`fPhysicalVolumeVector`)\n",
    "- It assigns importances that grow as powers of two along the shield:\n",
    "  $$\n",
    "  I_n = 2^n\n",
    "  $$\n",
    "  (with $n$ increasing as we move deeper in $+z$)\n",
    "\n",
    "**What does this mean for this hands-on:**\n",
    "- This importance gradient is what triggers **splitting / Russian roulette**\n",
    "- As a result, particle **weights change** and deep regions receive **more contributing histories**\n",
    "\n",
    "> You do *not* need to read the C++ to complete the hands-on.  \n",
    "> If you are curious, we can open the relevant function during the session break."
   ]
  },
  {
   "cell_type": "raw",
   "id": "44484b4a-90d7-4e9e-987e-57bea3b8633b",
   "metadata": {},
   "source": [
    "B01DetectorConstruction:: Creating Importance Store\n",
    "Going to assign importance: 1, to volume: cell_01\n",
    "Going to assign importance: 2, to volume: cell_02\n",
    "Going to assign importance: 4, to volume: cell_03\n",
    "Going to assign importance: 8, to volume: cell_04\n",
    "Going to assign importance: 16, to volume: cell_05\n",
    "Going to assign importance: 32, to volume: cell_06\n",
    "Going to assign importance: 64, to volume: cell_07\n",
    "Going to assign importance: 128, to volume: cell_08\n",
    "Going to assign importance: 256, to volume: cell_09\n",
    "Going to assign importance: 512, to volume: cell_10\n",
    "Going to assign importance: 1024, to volume: cell_11\n",
    "Going to assign importance: 2048, to volume: cell_12\n",
    "Going to assign importance: 4096, to volume: cell_13\n",
    "Going to assign importance: 8192, to volume: cell_14\n",
    "Going to assign importance: 16384, to volume: cell_15\n",
    "Going to assign importance: 32768, to volume: cell_16\n",
    "Going to assign importance: 65536, to volume: cell_17\n",
    "Going to assign importance: 131072, to volume: cell_18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e98306-521f-4a62-b453-ebb131d8ef37",
   "metadata": {},
   "source": [
    "## Confirming the importance map\n",
    "\n",
    "When running with `mode = 0`, the simulation prints the **importance values assigned to each cell**.\n",
    "\n",
    "üëâ This printout confirms that the simulation is running with the **intended importance map**, and that variance reduction is effectively enabled.\n",
    "\n",
    "---\n",
    "\n",
    "## How Geant4 uses importances  \n",
    "*(splitting and Russian roulette)*\n",
    "\n",
    "When a particle crosses a boundary from a region with importance  \n",
    "$ I_{\\text{old}} $ to $ I_{\\text{new}} $, Geant4 modifies the transport as follows.\n",
    "\n",
    "### Case A: $ I_{\\text{new}} > I_{\\text{old}} $ ‚Äî going deeper into the shield\n",
    "\n",
    "Geant4 increases sampling through **track splitting**:\n",
    "\n",
    "- The particle is replaced by several copies.\n",
    "- Each copy carries a **reduced statistical weight**.\n",
    "- The **expected contribution remains unbiased** when weighted estimators are used.\n",
    "\n",
    "This increases the number of particle histories reaching deep regions.\n",
    "\n",
    "---\n",
    "\n",
    "### Case B: $ I_{\\text{new}} < I_{\\text{old}} $ ‚Äî moving back toward less important regions\n",
    "\n",
    "Geant4 applies **Russian roulette**:\n",
    "\n",
    "- The particle may be killed with some probability.\n",
    "- If it survives, its weight is increased accordingly.\n",
    "- CPU time is not wasted tracking particles in regions that are unimportant for the ROI.\n",
    "\n",
    "---\n",
    "\n",
    "## What should change in the output\n",
    "\n",
    "Compared to the analog runs, you should now observe:\n",
    "\n",
    "- **Higher population in deep cells**, including the ROI (`cell_19`).\n",
    "- **Average track weight (`Av.Tr.WGT`) decreasing with depth**  \n",
    "  (splitting ‚Üí more tracks with smaller weights).\n",
    "- **`SL` and `SLW` are no longer equal**. **Why? ü§î** \n",
    "\n",
    "The quantity of interest remains the **ROI summary for `cell_19`**, in particular:\n",
    "- Mean(SLW)\n",
    "- StdErr(SLW)\n",
    "- Relative error (%)\n",
    "- Execution time\n",
    "- Figure of Merit (FOM)\n",
    "\n",
    "---\n",
    "\n",
    "## A note on stability\n",
    "\n",
    "Also monitor a **near-source cell** (e.g. `cell_03`):\n",
    "\n",
    "- Its mean value should remain **stable** when switching from analog to biased transport.\n",
    "- This will later help you distinguish:\n",
    "  - genuine variance reduction,\n",
    "  - from potential pathologies introduced by biased sampling.\n",
    "\n",
    "---\n",
    "\n",
    "## Run: Importance biasing enabled\n",
    "\n",
    "Now run the simulation with:\n",
    "\n",
    "- **Mode:** `mode = 0` (importance biasing ON)\n",
    "- **Number of events:**  \n",
    "  $$N = 100{,}000$$\n",
    "\n",
    "### Your task\n",
    "\n",
    "Compare this run against the **analog run with the same $N$**:\n",
    "\n",
    "- Per-cell tables:\n",
    "  - Population\n",
    "  - Av.Tr.WGT\n",
    "  - SL vs SLW\n",
    "- ROI summary for `cell_19`\n",
    "\n",
    "Quantify:\n",
    "- how much the **relative uncertainty** decreases,\n",
    "- how the **execution time** changes,\n",
    "- and how the **overall efficiency** improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beae1f96-5c05-4385-8e80-db1dda585332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-analog run: N = 100,000 events\n",
    "\n",
    "import os, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "base_dir  = Path.home() / \"CSC26/VR/hands_on_1\"\n",
    "build_dir = base_dir / \"build\"\n",
    "exe = build_dir / \"exampleB01\"  # estando en build/\n",
    "mode = 0\n",
    "N = 100000\n",
    "\n",
    "env = os.environ.copy()\n",
    "env[\"B01_MODE\"] = str(mode)\n",
    "\n",
    "subprocess.run([str(exe), str(mode), str(N)], env=env, check=True)\n",
    "\n",
    "print(\"\\nRun completed.\")\n",
    "print(\"Check the screen output and the files written in build/out/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9a72bb-31e5-49e0-a807-ea4d6c5d290f",
   "metadata": {},
   "source": [
    "## Inspecting the per-cell table (Variance Reduction vs Analog, same $N$)\n",
    "\n",
    "In the previous section you ran an **analog simulation** (`mode = -1`) with  \n",
    "$N = 100{,}000$ events.\n",
    "\n",
    "You have now run the **same simulation** with **importance biasing enabled**  \n",
    "(`mode = 0`) and the **same number of events**.\n",
    "\n",
    "‚úÖ **Nothing physical changed**  \n",
    "- Same geometry  \n",
    "- Same materials  \n",
    "- Same source  \n",
    "- Same physics list  \n",
    "- Same scoring definitions  \n",
    "\n",
    "‚ùó **Only the transport strategy changed**  \n",
    "Geant4 now uses **splitting and Russian roulette**, driven by **geometric importances**.\n",
    "\n",
    "Your task in this section is to inspect the **per-cell table** and understand:\n",
    "- what changed,\n",
    "- what stayed the same,\n",
    "- and *why* these changes occur.\n",
    "\n",
    "---\n",
    "\n",
    "### Focus cells for inspection\n",
    "\n",
    "To keep the analysis manageable, focus on two representative cells:\n",
    "\n",
    "- **`cell_03`**  \n",
    "  A *shallow* cell, close to the source.  \n",
    "  Many particle histories contribute here, so estimates should be **stable**.\n",
    "\n",
    "- **`cell_19`**  \n",
    "  The **deep-penetration region of interest (ROI)**.  \n",
    "  Very few analog histories reach this cell, which is exactly why variance reduction is needed.\n",
    "\n",
    "You should still glance at the **full per-cell table**, but your answers below\n",
    "should explicitly refer to **`cell_03`** and **`cell_19`**.\n",
    "\n",
    "---\n",
    "\n",
    "### Questions (short, qualitative answers)\n",
    "\n",
    "1. **Population**\n",
    "   - How does the `Population` in `cell_19` compare between analog and VR runs?\n",
    "   - Why does importance biasing change this quantity?\n",
    "\n",
    "2. **Average track weight**\n",
    "   - How does `Av.Tr.WGT` behave as a function of depth in the VR run?\n",
    "   - Why is this behavior expected when splitting is active?\n",
    "\n",
    "3. **SL vs SLW**\n",
    "   - Are `SL` and `SLW` still equal in the VR run?\n",
    "   - Which one is the **physically correct estimator** when variance reduction is enabled, and why?\n",
    "\n",
    "4. **Near vs deep cell**\n",
    "   - Does `cell_03` show large changes between analog and VR?\n",
    "   - Why should (or shouldn‚Äôt) this cell be sensitive to the transport strategy?\n",
    "\n",
    "---\n",
    "\n",
    "### Takeaway\n",
    "\n",
    "This table-level inspection helps you verify that:\n",
    "- variance reduction is **working as intended**,\n",
    "- deep regions receive more statistical weight,\n",
    "- and unbiased estimators (`SLW`) must be used once particle weights are no longer uniform.\n",
    "\n",
    "In the next step, you will quantify how these changes affect **uncertainty, runtime, and efficiency** in the ROI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dc026c-35a3-4d31-ac58-1db68b5f9144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load and display the per-cell tables for N=100k (mode = -1 and mode = 0) ---\n",
    "# This cell assumes you already defined:\n",
    "#   build_dir = Path.home() / \"CSC26/VR/hands_on_1/build\"\n",
    "#   out_dir   = build_dir / \"out\"\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "N_TARGET = 100000\n",
    "MODES = [-1, 0]\n",
    "\n",
    "def find_cells_table(out_dir: Path, mode: int, N: int) -> Path:\n",
    "    \"\"\"\n",
    "    Find the per-run table file:\n",
    "      out/b01_cells_mode{mode}_run{runID}_N{Nevt}.tsv\n",
    "    We don't assume runID, we just pick the newest match.\n",
    "    \"\"\"\n",
    "    pattern = f\"b01_cells_mode{mode}_run*_N{N}.tsv\"\n",
    "    matches = sorted(out_dir.glob(pattern), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    if not matches:\n",
    "        raise FileNotFoundError(\n",
    "            f\"No per-cell table found for mode={mode}, N={N} in {out_dir}\\n\"\n",
    "            f\"Expected something like: {pattern}\\n\"\n",
    "            f\"Tip: did you run with the env var set? e.g. B01_MODE={mode}\"\n",
    "        )\n",
    "    return matches[0]\n",
    "\n",
    "def read_cells_table(path: Path) -> pd.DataFrame:\n",
    "    # Our file has a header row and may have comment lines starting with '#'\n",
    "    df = pd.read_csv(path, sep=\"\\t\", comment=\"#\")\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "tables = {}\n",
    "\n",
    "print(f\"Looking for per-cell tables in: {out_dir}\")\n",
    "for mode in MODES:\n",
    "    path = find_cells_table(out_dir, mode, N_TARGET)\n",
    "    df = read_cells_table(path)\n",
    "    tables[mode] = (path, df)\n",
    "    print(f\"[OK] mode={mode}: {path.name}\")\n",
    "\n",
    "# Display full tables (scrollable) in the notebook\n",
    "for mode in MODES:\n",
    "    path, df = tables[mode]\n",
    "    print(f\"\\n=== Per-cell table (mode={mode}, N={N_TARGET}) ===\")\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b45479f-ba67-418c-9396-e9520bbe5a8b",
   "metadata": {},
   "source": [
    "### Columns to inspect and interpret\n",
    "\n",
    "For **each** of the following columns, compare **mode = -1** vs **mode = 0** (same N = 100k):\n",
    "\n",
    "1. **`Tr.Entering`**  \n",
    "2. **`Population`**  \n",
    "3. **`Av.Tr.WGT`**  \n",
    "4. **`SL`** and **`Sigma(SL)`**  \n",
    "5. **`SLW`** and **`Sigma(SLW)`**\n",
    "\n",
    "---\n",
    "\n",
    "### Write your observations (space for answers)\n",
    "\n",
    "Fill the table below using short bullet points.\n",
    "\n",
    "| Quantity | `cell_03`: what changes in mode 0 vs mode -1? | `cell_19`: what changes in mode 0 vs mode -1? | Why (one sentence) |\n",
    "|---|---|---|---|\n",
    "| Tr.Entering |  |  |  |\n",
    "| Population |  |  |  |\n",
    "| Av.Tr.WGT |  |  |  |\n",
    "| SL & Sigma(SL) |  |  |  |\n",
    "| SLW & Sigma(SLW) |  |  |  |\n",
    "\n",
    "---\n",
    "\n",
    "### Key conceptual questions\n",
    "\n",
    "1) **In analog runs (mode = -1), why do we typically observe `SL ‚âà SLW`?**  \n",
    "**Answer:**  \n",
    "\n",
    "2) **In VR runs (mode = 0), which estimator should be used as the unbiased result: `SL` or `SLW`? Why?**  \n",
    "**Answer:**  \n",
    "\n",
    "3) **Based on the table, does VR seem to ‚Äúmove statistics‚Äù toward deep cells? Point to two specific columns that support your answer.**  \n",
    "**Answer:**  \n",
    "\n",
    "---\n",
    "\n",
    "### Takeaway (one line)\n",
    "\n",
    "Write one sentence summarizing what you learned from the table inspection:\n",
    "\n",
    "**Takeaway:** ___________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6937b0-67b9-470e-a812-f123a46dc1fa",
   "metadata": {},
   "source": [
    "## Evaluating variance reduction: three essential checks\n",
    "\n",
    "So far, you have:\n",
    "\n",
    "- Understood the physical problem and the observable,\n",
    "- Run **analog** simulations and observed slow convergence in the deep ROI,\n",
    "- Enabled **importance biasing** and inspected how the per-cell statistics change.\n",
    "\n",
    "At this point, we move from *exploration* to **evaluation**.\n",
    "\n",
    "Whenever a variance reduction (VR) technique is introduced, it must be assessed carefully.\n",
    "Improving statistical precision alone is **not sufficient**.\n",
    "\n",
    "In this hands-on, we will evaluate importance biasing using **three fundamental checks**,\n",
    "which mirror best practice in real Monte Carlo work:\n",
    "\n",
    "---\n",
    "\n",
    "### The three checks\n",
    "\n",
    "**1. Accuracy (unbiasedness)**  \n",
    "Does variance reduction preserve the *physical expectation value* of the observable?\n",
    "\n",
    "> The mean value must remain consistent with the analog result within statistical uncertainty.\n",
    "\n",
    "---\n",
    "**2. Efficiency (cost vs precision)**  \n",
    "Does variance reduction reduce uncertainty *fast enough* to justify its additional cost?\n",
    "\n",
    "> This is where we quantify whether VR actually pays off in practice.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**3. Stability (pathologies and weight behavior)**  \n",
    "Does variance reduction introduce rare but extreme contributions that dominate the estimator?\n",
    "\n",
    "> We want to avoid estimators controlled by a handful of pathological events.\n",
    "\n",
    "---\n",
    "\n",
    "In the next cells, you will perform these three checks **step by step**, using the data\n",
    "you have already produced.\n",
    "\n",
    "We start with the most fundamental one:\n",
    "\n",
    "‚û°Ô∏è **Accuracy ‚Äî does variance reduction preserve the mean?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4913b6d1-f32f-4849-b3bf-4884c733278a",
   "metadata": {},
   "source": [
    "## Check 1 ‚Äî Accuracy: does variance reduction preserve the mean?\n",
    "\n",
    "So far, you have inspected **how the per-cell statistics change** when importance biasing is enabled:\n",
    "- more tracks reach deep cells,\n",
    "- particle weights are redistributed,\n",
    "- raw and weighted estimators no longer coincide.\n",
    "\n",
    "Now we move from *qualitative inspection* to a **quantitative check**.\n",
    "\n",
    "---\n",
    "\n",
    "### Why this check matters\n",
    "\n",
    "Variance reduction techniques are only useful if they satisfy a strict requirement:\n",
    "\n",
    "> **They must not change the physical expectation value of the observable.**\n",
    "\n",
    "Importance biasing is allowed to:\n",
    "- change how often certain histories are sampled,\n",
    "- introduce splitting and Russian roulette,\n",
    "- redistribute statistical weights,\n",
    "\n",
    "but it must **not bias the final result**.\n",
    "\n",
    "This property is called **accuracy** or **unbiasedness**.\n",
    "\n",
    "Because we work with *finite statistics*, two runs (analog and VR) will **never give exactly the same number**.\n",
    "What matters is whether the difference is **consistent with statistical fluctuations**.\n",
    "\n",
    "---\n",
    "\n",
    "### Comparing two estimates with uncertainties\n",
    "\n",
    "Let:\n",
    "- $\\mu_{-1}$ = estimate from the **analog** run (`mode = -1`)\n",
    "- $\\mu_{0}$  = estimate from the **variance-reduced** run (`mode = 0`)\n",
    "- $\\sigma_{-1}$ and $\\sigma_{0}$ = the **standard errors** of the estimator  \n",
    "  (the values printed as `Sigma(SLW)`)\n",
    "\n",
    "We first compute the difference:\n",
    "$$\n",
    "\\Delta = \\mu_{0}-\\mu_{-1}\n",
    "$$\n",
    "\n",
    "If the two runs are statistically independent, the uncertainty on this difference is:\n",
    "$$\n",
    "\\sigma_\\Delta = \\sqrt{\\sigma_0^2 + \\sigma_{-1}^2}\n",
    "$$\n",
    "\n",
    "**Why the square root?**  \n",
    "Because variances add for independent random variables:\n",
    "$$\n",
    "\\mathrm{Var}(X-Y)=\\mathrm{Var}(X)+\\mathrm{Var}(Y)\n",
    "$$\n",
    "\n",
    "We then define a normalized deviation (a ‚Äúz-score‚Äù):\n",
    "$$\n",
    "z = \\frac{\\mu_{0}-\\mu_{-1}}{\\sqrt{\\sigma_0^2+\\sigma_{-1}^2}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### How to interpret the result\n",
    "\n",
    "As a rule of thumb:\n",
    "- $|z|\\lesssim 2$:  \n",
    "  The two results are **statistically compatible** (no evidence of bias).\n",
    "- $|z|\\gg 2$:  \n",
    "  Potential problem ‚Äî could indicate bias, underestimated uncertainties, or insufficient statistics.\n",
    "\n",
    "This is **not** a strict hypothesis test, but a practical diagnostic.\n",
    "\n",
    "---\n",
    "\n",
    "### What to do\n",
    "\n",
    "1. Compare **SLW** (not SL) between `mode = -1` and `mode = 0`.  \n",
    "2. Start with a **near-source cell** such as `cell_03`, where analog statistics are good.  \n",
    "3. Inspect the **trend vs depth**:\n",
    "   - SLW should decrease smoothly as we move deeper into the shield.\n",
    "   - No systematic shift between analog and VR should appear.\n",
    "\n",
    "üëâ At this stage, focus on **consistency**, not exact numerical equality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389dcc8d-b76e-472d-a61f-b4f70f099f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# expects: tables dict from your previous cell:\n",
    "# tables[mode] = (path, df)\n",
    "# and df has at least: cellName, SLW_mean_mm, SLW_stderr_mm\n",
    "\n",
    "cell_acc = \"cell_03\"  # near-source cell for good analog stats\n",
    "\n",
    "df_m1 = tables[-1][1].copy()\n",
    "df_0  = tables[0][1].copy()\n",
    "\n",
    "# --- z-score at a reference cell ---\n",
    "m1 = df_m1.set_index(\"cellName\")\n",
    "m0 = df_0.set_index(\"cellName\")\n",
    "\n",
    "mu_m1 = float(m1.loc[cell_acc, \"SLW_mean_mm\"])\n",
    "se_m1 = float(m1.loc[cell_acc, \"SLW_stderr_mm\"])\n",
    "mu_0  = float(m0.loc[cell_acc, \"SLW_mean_mm\"])\n",
    "se_0  = float(m0.loc[cell_acc, \"SLW_stderr_mm\"])\n",
    "\n",
    "sigma_delta = np.sqrt(se_0**2 + se_m1**2)\n",
    "z = (mu_0 - mu_m1) / sigma_delta if sigma_delta > 0 else np.nan\n",
    "\n",
    "print(f\"Accuracy check at {cell_acc} (N=100k):\")\n",
    "print(f\"  mode=-1: mean(SLW)={mu_m1:.6g} mm, stderr={se_m1:.6g} mm\")\n",
    "print(f\"  mode= 0: mean(SLW)={mu_0:.6g} mm, stderr={se_0:.6g} mm\")\n",
    "print(f\"  Œî = {mu_0-mu_m1:.6g} mm\")\n",
    "print(f\"  œÉ_Œî = sqrt(œÉ0^2+œÉ-1^2) = {sigma_delta:.6g} mm\")\n",
    "print(f\"  z = Œî/œÉ_Œî = {z:.3f}   (|z| ‚â≤ 2 ‚Üí compatible)\")\n",
    "\n",
    "# --- helper: depth index from \"cell_XX\" ---\n",
    "def cell_index(name: str) -> int:\n",
    "    return int(name.split(\"_\")[1])\n",
    "\n",
    "df_m1[\"idx\"] = df_m1[\"cellName\"].apply(cell_index)\n",
    "df_0[\"idx\"]  = df_0[\"cellName\"].apply(cell_index)\n",
    "\n",
    "df_m1 = df_m1.sort_values(\"idx\")\n",
    "df_0  = df_0.sort_values(\"idx\")\n",
    "\n",
    "# --- Plot SLW mean vs depth for both modes ---\n",
    "plt.figure()\n",
    "plt.plot(df_m1[\"idx\"], df_m1[\"SLW_mean_mm\"], \"o-\", label=\"mode = -1 (analog), N=100k\")\n",
    "plt.plot(df_0[\"idx\"],  df_0[\"SLW_mean_mm\"],  \"o-\", label=\"mode = 0 (importance), N=100k\")\n",
    "plt.xlabel(\"Cell index (depth)\")\n",
    "plt.ylabel(\"Mean(SLW) [mm]\")\n",
    "plt.title(\"Accuracy diagnostic: SLW vs depth (same N, different modes)\")\n",
    "plt.yscale(\"log\")  # deep shielding trend is easier to see on a log scale\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Optional: also visualize uncertainties (stderr) vs depth\n",
    "plt.figure()\n",
    "plt.plot(df_m1[\"idx\"], df_m1[\"SLW_stderr_mm\"], \"o-\", label=\"mode = -1 stderr\")\n",
    "plt.plot(df_0[\"idx\"],  df_0[\"SLW_stderr_mm\"],  \"o-\", label=\"mode = 0 stderr\")\n",
    "plt.xlabel(\"Cell index (depth)\")\n",
    "plt.ylabel(\"StdErr(SLW) [mm]\")\n",
    "plt.title(\"Uncertainty vs depth (same N, different modes)\")\n",
    "plt.yscale(\"log\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de35c654-d7d4-4869-8a17-c34ef1b3120a",
   "metadata": {},
   "source": [
    "## From correctness to performance\n",
    "\n",
    "In **Check 1**, we verified something fundamental:\n",
    "importance biasing does **not** distort the physical mean of the observable.\n",
    "\n",
    "Once accuracy is established, the next question is purely practical:\n",
    "\n",
    "> **Is variance reduction actually worth it?**\n",
    "\n",
    "In real Monte Carlo work, we rarely care only about correctness.\n",
    "We care about **how much computer time is needed** to reach a given precision,\n",
    "especially in regions dominated by rare events.\n",
    "\n",
    "To answer that question quantitatively, we now introduce a standard performance metric:\n",
    "the **Figure of Merit (FOM)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665274e8-7fdb-4c61-a087-d77b73e41918",
   "metadata": {},
   "source": [
    "## Check 2 ‚Äî Efficiency (FOM improvement where it matters)\n",
    "\n",
    "**Goal:** show that importance biasing gives a higher **Figure of Merit (FOM)** in the region of interest (ROI).\n",
    "\n",
    "Recall the definition used by the code (for `cell_19`, using SLW):\n",
    "$$\n",
    "\\mathrm{FOM}=\\frac{1}{R^2\\,T}\n",
    "\\quad\\text{with}\\quad\n",
    "R=\\frac{\\sigma}{\\mu}\n",
    "$$\n",
    "- A good VR method typically **increases FOM**, even if it also increases run time \\(T\\),\n",
    "  because it reduces the relative uncertainty \\(R\\) a lot.\n",
    "\n",
    "### What to compare\n",
    "- **Analog (mode = -1), N = 10,000,000**  *(precomputed in SWAN)*\n",
    "- **Importance (mode = 0), N = 100,000** *(you just ran it)*\n",
    "\n",
    "Write down:\n",
    "- \\(R\\), \\(T\\), and FOM for both cases.\n",
    "- The ratio:\n",
    "$$\n",
    "\\frac{\\mathrm{FOM}_{\\mathrm{VR}}}{\\mathrm{FOM}_{\\mathrm{analog}}}\n",
    "$$\n",
    "Interpretation: ‚ÄúHow many times more efficient is VR in the ROI?‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db43c4b5-bb33-4f50-89a8-530c1eb89b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths from earlier\n",
    "build_dir = Path.home() / \"CSC26/VR/hands_on_1/build\"\n",
    "out_dir   = build_dir / \"out\"\n",
    "pre_dir   = Path.home() / \"CSC26/VR/hands_on_1/pcruns\"\n",
    "\n",
    "student_summary  = out_dir / \"b01_summary.tsv\"\n",
    "precomp_summary  = pre_dir / \"b01_summary_10M.tsv\"\n",
    "\n",
    "def read_summary(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, sep=\"\\t\")\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "df_sum = pd.concat([read_summary(student_summary),\n",
    "                    read_summary(precomp_summary)], ignore_index=True)\n",
    "\n",
    "# Keep only rows we need\n",
    "# We assume summary columns exist: mode, Nevt, time_real_s, roi_relerr, FOM_1_per_s\n",
    "needed = [\"mode\", \"Nevt\", \"time_real_s\", \"roi_relerr\", \"FOM_1_per_s\"]\n",
    "missing = [c for c in needed if c not in df_sum.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns in summary: {missing}\")\n",
    "\n",
    "def pick_row(df, mode, Nevt):\n",
    "    sub = df[(df[\"mode\"].astype(str).str.strip() == str(mode)) & (df[\"Nevt\"] == Nevt)].copy()\n",
    "    if len(sub) == 0:\n",
    "        raise RuntimeError(f\"No summary row found for mode={mode}, Nevt={Nevt}\")\n",
    "    # If multiple, take the last one appended\n",
    "    return sub.iloc[-1]\n",
    "\n",
    "row_vr   = pick_row(df_sum, 0, 100000)\n",
    "row_10m  = pick_row(df_sum, -1, 10000000)\n",
    "\n",
    "def fmt_row(r):\n",
    "    R = float(r[\"roi_relerr\"])\n",
    "    T = float(r[\"time_real_s\"])\n",
    "    F = float(r[\"FOM_1_per_s\"])\n",
    "    return R, T, F\n",
    "\n",
    "R_vr,  T_vr,  F_vr  = fmt_row(row_vr)\n",
    "R_10m, T_10m, F_10m = fmt_row(row_10m)\n",
    "\n",
    "print(\"Efficiency check (ROI = cell_19, using SLW):\")\n",
    "print(f\"  VR   mode=0,   N=1e5 : R={100*R_vr:.3g} %,  T={T_vr:.3g} s,  FOM={F_vr:.3g} 1/s\")\n",
    "print(f\"  Analog mode=-1, N=1e7 : R={100*R_10m:.3g} %, T={T_10m:.3g} s, FOM={F_10m:.3g} 1/s\")\n",
    "print()\n",
    "print(f\"  FOM improvement factor = FOM_VR / FOM_Analog = {F_vr/F_10m:.3g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9376058d-44dd-41e6-97dd-796f0737e081",
   "metadata": {},
   "source": [
    "## Why a stability check is still needed\n",
    "\n",
    "So far we have shown that importance biasing:\n",
    "\n",
    "- Preserves the physical mean (accuracy)\n",
    "- Improves performance in the ROI (efficiency)\n",
    "\n",
    "However, **neither of these guarantees a healthy Monte Carlo simulation**.\n",
    "\n",
    "A variance reduction setup can be:\n",
    "- Unbiased\n",
    "- Efficient on average\n",
    "  \n",
    "and still be **dangerous**, if it relies on very rare events with extremely large weights.\n",
    "\n",
    "Such pathologies often reveal themselves only when inspecting\n",
    "**weight behavior and consistency across regions**.\n",
    "\n",
    "This final check focuses on **stability**, using simple diagnostics available in this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf83994-30b8-44e3-b9cb-7874a7fa138e",
   "metadata": {},
   "source": [
    "## Check 3 ‚Äî Stability (weight behavior + diagnostic consistency)\n",
    "\n",
    "**Goal:** detect \"pathological\" VR configurations that create rare **huge weights** (\"spikes\"),\n",
    "which can ruin statistics even if the mean is unbiased.\n",
    "\n",
    "In our simplified setup, we use two practical diagnostics:\n",
    "\n",
    "### (A) Weight trend with depth\n",
    "Look at `Av.Tr.WGT` versus cell index:\n",
    "- In mode = 0, as importances increase with depth, splitting produces **more tracks with smaller weights**.\n",
    "- So `Av.Tr.WGT` should generally **decrease** with depth.\n",
    "\n",
    "### (B) Near-source stability across modes\n",
    "Check that in near-source cells (e.g. `cell_02`‚Äì`cell_05`) the results are well-behaved:\n",
    "- `SLW_mean_mm` should remain consistent with analog within uncertainties (accuracy-like check)\n",
    "- `Sigma(SLW)` should not explode.\n",
    "\n",
    "> In a full production exercise, we would also histogram weights and look for spikes.\n",
    "> Here we approximate this with the *average weight trend* and near-source consistency.\n",
    "\n",
    "Write down:\n",
    "- `Av.Tr.WGT` in `cell_03` and in the deepest shield cell (e.g. `cell_18`) for mode 0.\n",
    "- Is the trend monotonic decreasing? Any weird jumps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bb5241-37a0-46f6-a7d5-5c36989fd6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use the per-cell tables we already loaded for N=100k\n",
    "df_m1 = tables[-1][1].copy()\n",
    "df_0  = tables[0][1].copy()\n",
    "\n",
    "# Extract cell index from the name \"cell_XX\"\n",
    "def cell_index(name):\n",
    "    return int(name.split(\"_\")[1])\n",
    "\n",
    "df_0[\"idx\"] = df_0[\"cellName\"].apply(cell_index)\n",
    "df_0 = df_0.sort_values(\"idx\")\n",
    "\n",
    "# --- Plot average weight vs depth (mode 0) ---\n",
    "plt.figure()\n",
    "plt.plot(df_0[\"idx\"], df_0[\"AvTrWgt\"], \"o-\")\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"Cell index (depth)\")\n",
    "plt.ylabel(\"Av.Tr.WGT (mode 0)\")\n",
    "plt.title(\"Stability diagnostic: average weight vs depth (importance biasing)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- Print a quick table for near-source cells (02-05) comparing mode -1 vs 0 ---\n",
    "focus = [f\"cell_{i:02d}\" for i in range(2, 6)]\n",
    "\n",
    "cmp_cols = [\"SLW_mean_mm\", \"SLW_stderr_mm\", \"AvTrWgt\", \"Population\", \"TrEntering\"]\n",
    "cmp_cols = [c for c in cmp_cols if c in df_0.columns]\n",
    "\n",
    "m1 = df_m1.set_index(\"cellName\").loc[focus, cmp_cols].add_prefix(\"mode_-1_\")\n",
    "m0 = df_0.set_index(\"cellName\").loc[focus, cmp_cols].add_prefix(\"mode_0_\")\n",
    "cmp = pd.concat([m1, m0], axis=1)\n",
    "\n",
    "print(\"Near-source cells consistency (N=100k):\")\n",
    "display(cmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91af4366-abf7-4335-a457-f708cce4abf4",
   "metadata": {},
   "source": [
    "Up to now, we have evaluated variance reduction using three qualitative checks:\n",
    "\n",
    "1. **Accuracy** ‚Äî does it preserve the physical mean?\n",
    "2. **Efficiency** ‚Äî does it reduce uncertainty faster than brute force?\n",
    "3. **Stability** ‚Äî does it behave well at the event and weight level?\n",
    "\n",
    "We now summarize these conclusions **quantitatively**, using the same metrics\n",
    "you have already seen: relative uncertainty, runtime, and Figure of Merit (FOM)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cfa229-7299-4774-bf5c-fefff63335a6",
   "metadata": {},
   "source": [
    "## Final quantitative comparison\n",
    "\n",
    "We now quantify *why* variance reduction exists by comparing the\n",
    "**region of interest (`cell_19`)** using three metrics:\n",
    "\n",
    "- Relative uncertainty: $R = \\sigma/\\mu$\n",
    "- Wall time: $T$ (seconds)\n",
    "- Figure of Merit:\n",
    "$$\n",
    "\\mathrm{FOM} = \\frac{1}{R^2\\,T}\n",
    "$$\n",
    "\n",
    "**Important:** a higher FOM means *more precision per unit of CPU time*.\n",
    "\n",
    "---\n",
    "\n",
    "### (a) Fair comparison: same statistics, different transport  \n",
    "**Analog 100k vs VR 100k**\n",
    "\n",
    "This is the cleanest possible comparison:\n",
    "\n",
    "- Same geometry, materials, physics, scoring, and machine\n",
    "- Same number of events ($N = 100{,}000$)\n",
    "- **Only the transport strategy changes**\n",
    "\n",
    "**Your task:**\n",
    "1. Extract $R$, $T$, and FOM for:\n",
    "   - Analog 100k\n",
    "   - VR 100k\n",
    "2. Compute the ratios:\n",
    "   - $R_{\\text{analog}} / R_{\\text{VR}}$\n",
    "   - $T_{\\text{VR}} / T_{\\text{analog}}$\n",
    "   - $\\mathrm{FOM}_{\\text{VR}} / \\mathrm{FOM}_{\\text{analog}}$\n",
    "\n",
    "Interpretation:\n",
    "- How much uncertainty reduction do you get?\n",
    "- What is the runtime penalty?\n",
    "- Is the trade-off worth it?\n",
    "\n",
    "---\n",
    "\n",
    "### (b) Practical comparison: same environment, different $N$  \n",
    "**Analog 10M vs VR 100k**\n",
    "\n",
    "This is the comparison that shows why VR is necessary.\n",
    "\n",
    "- Analog 10M was run in the **same SWAN environment**\n",
    "- It is provided precomputed because it is too slow to run live\n",
    "\n",
    "**Your task:**\n",
    "1. Compare:\n",
    "   - Analog 10M\n",
    "   - VR 100k\n",
    "2. Compute:\n",
    "   - Event ratio: $N_{10\\text{M}} / N_{100\\text{k}}$\n",
    "   - $R_{10\\text{M}} / R_{\\text{VR}}$\n",
    "   - $T_{10\\text{M}} / T_{\\text{VR}}$\n",
    "   - $\\mathrm{FOM}_{\\text{VR}} / \\mathrm{FOM}_{10\\text{M}}$\n",
    "\n",
    "Interpretation:\n",
    "- How many more events does analog need to reach similar precision?\n",
    "- How many times more efficient is VR in the ROI?\n",
    "\n",
    "---\n",
    "\n",
    "‚û°Ô∏è Run the next code cell to load the summary files and compute these comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca0d041-7626-4d80-98e6-9fcec499e1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "build_dir = Path.home() / \"CSC26/VR/hands_on_1/build\"\n",
    "out_dir   = build_dir / \"out\"\n",
    "pre_dir   = Path.home() / \"CSC26/VR/hands_on_1/pcruns\"\n",
    "\n",
    "student_summary  = out_dir / \"b01_summary.tsv\"\n",
    "precomp_summary  = pre_dir / \"b01_summary_10M.tsv\"\n",
    "\n",
    "def read_summary_tsv(path: Path) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing summary file: {path}\")\n",
    "    df = pd.read_csv(path, sep=\"\\t\")\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "dfs = [read_summary_tsv(student_summary), read_summary_tsv(precomp_summary)]\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Expected columns from RunAction\n",
    "col_mode = \"mode\"\n",
    "col_N    = \"Nevt\"\n",
    "col_T    = \"time_real_s\"\n",
    "col_R    = \"roi_relerr\"\n",
    "col_F    = \"FOM_1_per_s\"\n",
    "\n",
    "# Basic checks\n",
    "for c in [col_N, col_T, col_R]:\n",
    "    if c not in df.columns:\n",
    "        raise ValueError(f\"Missing required column: {c}\")\n",
    "\n",
    "# Compute FOM if not present\n",
    "if col_F not in df.columns:\n",
    "    df[col_F] = 1.0 / (df[col_R]**2 * df[col_T])\n",
    "\n",
    "# Keep only rows with finite R and T\n",
    "df = df[np.isfinite(df[col_R]) & np.isfinite(df[col_T])].copy()\n",
    "\n",
    "# Show a compact table (ROI only)\n",
    "show_cols = [c for c in [\"mode\", col_N, col_T, col_R, col_F] if c in df.columns]\n",
    "roi = df[show_cols].copy()\n",
    "roi[\"R_%\"] = 100.0 * roi[col_R]\n",
    "roi = roi.sort_values([col_mode, col_N], key=lambda s: s.astype(str))\n",
    "\n",
    "print(\"ROI summary rows available:\")\n",
    "display(roi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b3b35e-ec99-47f7-8628-09e2abda609e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_run(df, mode, N):\n",
    "    sub = df.copy()\n",
    "    if col_mode in sub.columns:\n",
    "        sub = sub[sub[col_mode].astype(str).str.strip() == str(mode)]\n",
    "    sub = sub[sub[col_N].astype(float) == float(N)]\n",
    "    if len(sub) == 0:\n",
    "        raise ValueError(f\"No row found for mode={mode}, N={N}\")\n",
    "    return sub.tail(1).iloc[0]  # if duplicates exist, use last appended\n",
    "\n",
    "def fmt(x): \n",
    "    return f\"{x:.6g}\"\n",
    "\n",
    "# Grab runs\n",
    "analog_100k = pick_run(df, -1, 100_000)\n",
    "vr_100k     = pick_run(df,  0, 100_000)\n",
    "analog_10M  = pick_run(df, -1, 10_000_000)\n",
    "\n",
    "# (a) Fair comparison: same N\n",
    "R_ratio_a = float(analog_100k[col_R]) / float(vr_100k[col_R])\n",
    "T_ratio_a = float(vr_100k[col_T]) / float(analog_100k[col_T])\n",
    "F_ratio_a = float(vr_100k[col_F]) / float(analog_100k[col_F])\n",
    "\n",
    "print(\"\\n(a) Fair comparison (N = 100k): Analog vs VR\")\n",
    "print(f\"  R_analog / R_VR        = {fmt(R_ratio_a)}\")\n",
    "print(f\"  T_VR / T_analog        = {fmt(T_ratio_a)}\")\n",
    "print(f\"  FOM_VR / FOM_analog    = {fmt(F_ratio_a)}\")\n",
    "\n",
    "# (b) Dramatic comparison: 10M vs 100k\n",
    "events_ratio_b = float(analog_10M[col_N]) / float(vr_100k[col_N])\n",
    "R_ratio_b      = float(analog_10M[col_R]) / float(vr_100k[col_R])\n",
    "T_ratio_b      = float(analog_10M[col_T]) / float(vr_100k[col_T])\n",
    "F_ratio_b      = float(vr_100k[col_F]) / float(analog_10M[col_F])\n",
    "\n",
    "print(\"\\n(b) VR comparison: Analog 10M vs VR 100k\")\n",
    "print(f\"  N_10M / N_100k         = {fmt(events_ratio_b)}\")\n",
    "print(f\"  R_10M / R_VR100k       = {fmt(R_ratio_b)}\")\n",
    "print(f\"  T_10M / T_VR100k       = {fmt(T_ratio_b)}\")\n",
    "print(f\"  FOM_VR100k / FOM_10M   = {fmt(F_ratio_b)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc45511-a6af-49e8-9d35-bfffe872ab40",
   "metadata": {},
   "source": [
    "## Final conclusion\n",
    "\n",
    "This is the final synthesis of the hands-on. Use the numerical results you obtained above.\n",
    "Do **not** answer with vague statements.\n",
    "\n",
    "---\n",
    "\n",
    "### (a) Comparison - Analog 100k vs VR 100k \n",
    "\n",
    "1. **Relative uncertainty:** did it improve? By what factor?  \n",
    "   ‚Üí $R_{\\text{analog}} / R_{\\text{VR}} =$ ________\n",
    "\n",
    "2. **Runtime:** did VR cost extra time? By what factor?  \n",
    "   ‚Üí $T_{\\text{VR}} / T_{\\text{analog}} =$ ________\n",
    "\n",
    "3. **Efficiency:** did the FOM improve? By what factor?  \n",
    "   ‚Üí $\\mathrm{FOM}_{\\text{VR}} / \\mathrm{FOM}_{\\text{analog}} =$ ________\n",
    "\n",
    "**One-sentence quantitative takeaway:**  \n",
    "\n",
    "Write **one sentence** that includes at least **one numerical factor**\n",
    "\n",
    "(e.g. ‚ÄúVR reduced the relative error by a factor X at comparable cost‚Äù).\n",
    "\n",
    "---\n",
    "\n",
    "### (b) Comparison - Analog 10M vs VR 100k\n",
    "\n",
    "1. **Events:** how many fewer events did VR use?  \n",
    "   ‚Üí $N_{10M} / N_{100k} =$ ________√ó\n",
    "\n",
    "2. **Precision:** how does relative uncertainty compare?  \n",
    "   ‚Üí $R_{10M} / R_{\\text{VR}} =$ ________√ó\n",
    "\n",
    "3. **Efficiency:** compare FOMs  \n",
    "   ‚Üí $\\mathrm{FOM}_{\\text{VR}} / \\mathrm{FOM}_{10M} =$ ________√ó\n",
    "\n",
    "**One-sentence quantitative takeaway:**  \n",
    "\n",
    "Write **one sentence** that includes at least **one numerical factor**\n",
    "\n",
    "(e.g. ‚ÄúVR reduced the relative error by a factor X at comparable cost‚Äù).\n",
    "\n",
    "---\n",
    "\n",
    "### Link back to the three checks (be specific)\n",
    "\n",
    "- **Accuracy:**  \n",
    "  State whether the VR and analog results are statistically compatible,\n",
    "  and mention the criterion you used (e.g. z-score, overlap of uncertainties).\n",
    "\n",
    "- **Efficiency:**  \n",
    "  State by how much the FOM improved and what dominated the gain\n",
    "  (variance reduction vs runtime increase).\n",
    "\n",
    "- **Stability (weights / behaviour):**  \n",
    "  State whether the VR estimator showed smooth behaviour or signs of instability,\n",
    "  and point to at least one observable (e.g. Av.Tr.WGT trend, absence of spikes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a93fec-77b1-4c52-add6-96e59a2e4cf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
